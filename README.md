# RAGTESTERCLI

A unified CLI tool for evaluating Retrieval-Augmented Generation (RAG) pipelines with support for multiple LLM providers and evaluation metrics.

## Purpose

This tool provides a unified interface for running RAG evaluation metrics from popular open-source tools like RAGAS and RAGChecker, with support for multiple LLM providers.

## Features

- **Unified CLI Interface**: Typer-based command-line interface
- **Multiple Metrics**: Support for RAGAS and RAGChecker evaluations
- **Multi-Provider LLM Support**: OpenAI, Anthropic, Google, OpenRouter, and more
- **Environment Variable Support**: Secure API key management via .env files
- **Flexible Output**: JSON and table formats
- **Extensible Design**: Easy to add new metrics and providers

## Quick Start

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd RAGTESTERCLI
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your API keys (optional - can also use CLI flags):
```bash
# Create .env file
cp .env.example .env
# Edit .env with your API keys
```

### Basic Usage

```bash
# Run faithfulness evaluation with OpenAI
python cli.py test --input examples/test.json --metric faithfulness_ragas --llm-model gpt-4

# Run hallucination detection with OpenRouter
python cli.py test --input examples/test.json --metric hallucination_ragchecker --llm-model anthropic/claude-3-haiku --api-base https://openrouter.ai/api/v1

# Using environment variables (.env file)
python cli.py test --input examples/test.json --metric faithfulness_ragas --llm-model gpt-3.5-turbo

# Output as table format
python cli.py test --input examples/test.json --metric faithfulness_ragas --output-format table

# Save results to file
python cli.py test --input examples/test.json --metric faithfulness_ragas --output results.json
```

### List Available Metrics

```bash
python cli.py list-metrics
```

## Supported Metrics

| Tool | Metric | Description |
|------|--------|-------------|
| RAGAS | `faithfulness_ragas` | Measures if answer is grounded in context |
| RAGChecker | `hallucination_ragchecker` | Detects hallucinated information |

## Input Format

The tool expects a JSON file with the following structure:

```json
[
  {
    "question": "When was GPT-4 released?",
    "context": ["GPT-4 was released in March 2023 by OpenAI."],
    "answer": "GPT-4 came out in 2023."
  }
]
```

Required fields:
- `question`: The question being answered
- `context`: Array of context strings or single string
- `answer`: The answer to evaluate

## Configuration

Create a `ragtester.yaml` file for custom configuration:

```yaml
llm:
  provider: openai
  model: gpt-4
  api_key: ${OPENAI_API_KEY}
  temperature: 0.0
  max_tokens: 1000

output:
  format: json
  include_reasoning: false
  decimal_places: 3
```

## ğŸ“ Project Structure

```
RAGTESTERCLI/
â”œâ”€â”€ cli.py                          # Main CLI interface
â”œâ”€â”€ config.py                       # Configuration management
â”œâ”€â”€ runner.py                       # Evaluation orchestration
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ base_evaluator.py          # Base evaluator class
â”‚   â”œâ”€â”€ ragas_faithfulness.py      # RAGAS faithfulness metric
â”‚   â””â”€â”€ ragchecker_hallucination.py # RAGChecker hallucination metric
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ openai_client.py           # OpenAI API wrapper
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ test.json                  # Sample test data
â””â”€â”€ requirements.txt               # Dependencies
```

## ğŸ§© Adding New Metrics

1. Create a new evaluator in `evaluators/`:
```python
from .base_evaluator import BaseEvaluator

class MyCustomEvaluator(BaseEvaluator):
    def evaluate(self, data):
        # Your evaluation logic here
        return score
```

2. Register it in `evaluators/__init__.py`:
```python
EVALUATORS = {
    'my_custom_metric': MyCustomEvaluator,
    # ... other evaluators
}
```

## ğŸ“ˆ Output Example

### JSON Format
```json
{
  "metric": "faithfulness_ragas",
  "summary": {
    "total_items": 3,
    "average_score": 0.867,
    "min_score": 0.750,
    "max_score": 0.950,
    "execution_time_seconds": 12.34
  },
  "scores": [0.950, 0.850, 0.750]
}
```

### Table Format
```
EVALUATION SUMMARY
==================================================
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Value                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metric              â”‚ faithfulness_ragas      â”‚
â”‚ Total Items         â”‚ 3                       â”‚
â”‚ Average Score       â”‚ 0.867                   â”‚
â”‚ Min Score           â”‚ 0.750                   â”‚
â”‚ Max Score           â”‚ 0.950                   â”‚
â”‚ Execution Time (s)  â”‚ 12.34                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Development

This is an MVP focused on:
- âœ… Plug-and-play metric testing
- âœ… LLM-driven scoring
- âœ… Clean CLI interface  
- âœ… Expandable backend

Future enhancements could include:
- Additional metric providers (LynX, HaluBench, DeepEval)
- More LLM providers (Claude, Mistral)
- Batch processing optimizations
- Web interface
- Integration with CI/CD pipelines

## ğŸ“„ License

[Add your license here]

## ğŸ¤ Contributing

[Add contribution guidelines here] 